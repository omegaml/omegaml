""" Native storage for OmegaML using mongodb as the storage layer

An OmegaStore instance is a MongoDB database. It has at least the
metadata collection which lists all objects stored in it. A metadata
document refers to the following types of objects (metadata.kind):

* pandas.dfrows - a Pandas DataFrame stored as a collection of rows
* sklearn.joblib - a scikit learn estimator/pipline dumped using joblib.dump()
* python.data - an arbitrary python dict, tuple, list stored as a document

Note that storing Pandas and scikit learn objects requires the availability
of the respective packages. If either can not be imported, the OmegaStore
degrades to a python.data store only. It will still .list() and get() any
object, however reverts to pure python objects. In this case it is up
to the client to convert the data into an appropriate format for processing.

Pandas and scikit-learn objects can only be stored if these packages are
availables. put() raises a TypeError if you pass such objects and these
modules cannot be loaded.

All data are stored within the same mongodb, in per-object collections
as follows:

    * .metadata
        all metadata. each object is one document,
        See **omegaml.documents.Metadata** for details
    * .<bucket>.files
        this is the GridFS instance used to store
        blobs (models, numpy, hdf). The actual file name
        will be <prefix>/<name>.<ext>, where ext is
        optionally generated by put() / get().
    * .<bucket>.<prefix>.<name>.data
        every other dataset is stored in a separate
        collection (dataframes, dicts, lists, tuples).
        Any forward slash in prefix is ignored (e.g. 'data/'
        becomes 'data')

    DataFrames by default are stored in their own collection, every
    row becomes a document. To store dataframes as a binary file,
    use `put(...., as_hdf=True).` `.get()` will always return a dataframe.

    Python dicts, lists, tuples are stored as a single document with
    a `.data` attribute holding the JSON-converted representation. `.get()`
    will always return the corresponding python object of .data.

    Models are joblib.dump()'ed and ziped prior to transferring into
    GridFs. .get() will always unzip and joblib.load() before returning
    the model. Note this requires that the process using .get() supports
    joblib as well as all python classes referred to. If joblib is not
    supported, .get() returns a file-like object.

    The .metadata entry specifies the format used to store each
    object as well as it's location:

    * metadata.kind
        the type of object
    * metadata.name
        the name of the object, as given on put()
    * metadata.gridfile
        the gridfs object (if any, null otherwise)
    * metadata.collection
        the name of the collection
    * metadata.attributes
        arbitrary custom attributes set in
        put(attributes=obj). This is used e.g. by
        OmegaRuntime's fit() method to record the
        data used in the model's training.

    **.put()** and **.get()** use helper methods specific to the type in
    object's type and metadata.kind, respectively. In the future
    a plugin system will enable extension to other types.
"""
from __future__ import absolute_import

import logging
import os
import shutil
import warnings
import weakref
from uuid import uuid4

import bson
import gridfs
from mongoengine.connection import disconnect, connect, _connections, get_db
from mongoengine.errors import DoesNotExist
from mongoengine.queryset.visitor import Q

from omegaml.documents import make_Metadata, MDREGISTRY
from omegaml.mongoshim import sanitize_mongo_kwargs, waitForConnection
from omegaml.util import load_class, extend_instance, ensure_index, PickableCollection, signature
from omegaml.util import settings as omega_settings, urlparse

logger = logging.getLogger(__name__)


class OmegaStore(object):
    """ The storage backend for models and data

    .. versionchanged:: NEXT
        refactored all methods handling Python and Pandas datatypes to omegaml.backends.coreobjects.CoreObjectsBackend
    """

    def __init__(self, mongo_url=None, bucket=None, prefix=None, kind=None, defaults=None, dbalias=None):
        """
        :param mongo_url: the mongourl to use for the gridfs
        :param bucket: the mongo collection to use for gridfs
        :param prefix: the path prefix for files. defaults to blank
        :param kind: the kind or list of kinds to limit this store to
        """
        self.defaults = defaults or omega_settings()
        self.mongo_url = mongo_url or self.defaults.OMEGA_MONGO_URL
        self.bucket = bucket or self.defaults.OMEGA_MONGO_COLLECTION
        self._fs_collection = self._ensure_fs_collection()
        self._fs = None
        self._tmppath = os.path.join(self.defaults.OMEGA_TMP, uuid4().hex)
        self.prefix = prefix or ''
        self.force_kind = kind
        self._Metadata_cls = None
        # don't initialize db here to avoid using the default settings
        # otherwise Metadata will already have a connection and not use
        # the one provided in override_settings
        self._db = None
        self._dbalias = dbalias
        # add backends and mixins
        self._apply_mixins()
        # register backends
        self.register_backends()
        # register finalizer for cleanup
        weakref.finalize(self, self._cleanup, repr(self), tmppath=str(self._tmppath))

    def __repr__(self):
        return 'OmegaStore(bucket={}, prefix={})'.format(self.bucket, self.prefix)

    def __equal__(self, other):
        """test for equality of OmegaStore instances

        Args:
            other: OmegaStore instance

        Returns:
            True if other is the same database, same bucket, same prefix
        """
        return self.mongo_url == other.mongo_url and self.bucket == other.bucket and self.prefix == other.prefix

    @property
    def tmppath(self):
        """
        return an instance-specific temporary path
        """
        os.makedirs(self._tmppath, exist_ok=True)
        return self._tmppath

    @property
    def mongodb(self):
        """
        Returns a mongo database object
        """
        if self._db is not None:
            return self._db
        # parse salient parts of mongourl, e.g.
        # mongodb://user:password@host/dbname
        self.parsed_url = urlparse.urlparse(self.mongo_url)
        self.database_name = self.parsed_url.path[1:]
        host = self.parsed_url.netloc
        scheme = self.parsed_url.scheme
        username, password = None, None
        if '@' in host:
            creds, host = host.split('@', 1)
            if ':' in creds:
                username, password = creds.split(':')
        # connect via mongoengine
        #
        # note this uses a MongoClient in the background, with pooled
        # connections. there are multiprocessing issues with pymongo:
        # http://api.mongodb.org/python/3.2/faq.html#using-pymongo-with-multiprocessing
        # connect=False is due to https://jira.mongodb.org/browse/PYTHON-961
        # this defers connecting until the first access
        # serverSelectionTimeoutMS=2500 is to fail fast, the default is 30000
        #
        # use an instance specific alias, note that access to Metadata and
        # QueryCache must pass the very same alias
        self._dbalias = alias = self._dbalias or 'omega-{}'.format(uuid4().hex)
        # always disconnect before registering a new connection because
        # mongoengine.connect() forgets all connection settings upon disconnect
        if alias not in _connections:
            disconnect(alias)
            connection = connect(alias=alias, db=self.database_name,
                                 host=f'{scheme}://{host}',
                                 username=username,
                                 password=password,
                                 connect=False,
                                 authentication_source='admin',
                                 **sanitize_mongo_kwargs(self.defaults.OMEGA_MONGO_SSL_KWARGS),
                                 )
            # since PyMongo 4, connect() no longer waits for connection
            waitForConnection(connection)
        self._db = get_db(alias)
        return self._db

    @property
    def _Metadata(self):
        if self._Metadata_cls is None:
            # hack to localize metadata
            db = self.mongodb
            self._Metadata_cls = make_Metadata(db_alias=self._dbalias,
                                               collection=self._fs_collection)
        return self._Metadata_cls

    @property
    def fs(self):
        """
        Retrieve a gridfs instance using url and collection provided

        :return: a gridfs instance
        """
        if self._fs is not None:
            return self._fs
        self._fs = gridfs.GridFS(self.mongodb, collection=self._fs_collection)
        self._ensure_fs_index(self._fs)
        return self._fs

    def metadata(self, name=None, bucket=None, prefix=None, version=-1, **kwargs):
        """
        Returns a metadata document for the given entry name
        """
        # FIXME: version attribute does not do anything
        # FIXME: metadata should be stored in a bucket-specific collection
        # to enable access control, see https://docs.mongodb.com/manual/reference/method/db.create
        #
        # Role/#db.createRole
        db = self.mongodb
        fs = self.fs
        prefix = prefix or self.prefix
        bucket = bucket or self.bucket
        # Meta is to silence lint on import error
        Meta = self._Metadata
        return Meta.objects(name=str(name), prefix=prefix, bucket=bucket).no_cache().first()

    def make_metadata(self, name, kind, bucket=None, prefix=None, **kwargs):
        """
        create or update a metadata object

        this retrieves a Metadata object if it exists given the kwargs. Only
        the name, prefix and bucket arguments are considered

        for existing Metadata objects, the attributes kw is treated as follows:

        * attributes=None, the existing attributes are left as is
        * attributes={}, the attributes value on an existing metadata object
          is reset to the empty dict
        * attributes={ some : value }, the existing attributes are updated

        For new metadata objects, attributes defaults to {} if not specified,
        else is set as provided.

        :param name: the object name
        :param bucket: the bucket, optional, defaults to self.bucket
        :param prefix: the prefix, optional, defaults to self.prefix

        """
        # TODO kept _make_metadata for backwards compatibility.
        return self._make_metadata(name, bucket=bucket, prefix=prefix,
                                   kind=kind, **kwargs)

    def _make_metadata(self, name=None, bucket=None, prefix=None, **kwargs):
        """
        create or update a metadata object

        this retrieves a Metadata object if it exists given the kwargs. Only
        the name, prefix and bucket arguments are considered

        for existing Metadata objects, the attributes kw is treated as follows:

        * attributes=None, the existing attributes are left as is
        * attributes={}, the attributes value on an existing metadata object
        is reset to the empty dict
        * attributes={ some : value }, the existing attributes are updated

        For new metadata objects, attributes defaults to {} if not specified,
        else is set as provided.

        :param name: the object name
        :param bucket: the bucket, optional, defaults to self.bucket
        :param prefix: the prefix, optional, defaults to self.prefix
        """
        bucket = bucket or self.bucket
        prefix = prefix or self.prefix
        meta = self.metadata(name=name,
                             prefix=prefix,
                             bucket=bucket)
        if meta:
            dict_fields = 'attributes', 'kind_meta'
            for k, v in kwargs.items():
                if k in dict_fields and v is not None and len(v) > 0:
                    previous = getattr(meta, k, {})
                    previous.update(v)
                    setattr(meta, k, previous)
                elif k in dict_fields and v is not None and len(v) == 0:
                    setattr(meta, k, {})
                elif k in dict_fields and v is None:
                    # ignore non specified attributes
                    continue
                else:
                    # by default set whatever attribute is provided
                    setattr(meta, k, v)
        else:
            meta = self._Metadata(name=name, bucket=bucket, prefix=prefix,
                                  **kwargs)
        return meta

    def _drop_metadata(self, name=None, **kwargs):
        # internal method to delete meta data of an object
        meta = self.metadata(name, **kwargs)
        if meta is not None:
            meta.delete()

    def collection(self, name=None, bucket=None, prefix=None):
        """
        Returns a mongo db collection as a datastore

        If there is an existing object of name, will return the .collection
        of the object. Otherwise returns the collection according to naming
        convention {bucket}.{prefix}.{name}.datastore

        :param name: the collection to use. if none defaults to the
            collection name given on instantiation. the actual collection name
            used is always prefix + name + '.data'
        """
        # see if we have a known object and a collection for that, if not define it
        meta = self.metadata(name, bucket=bucket, prefix=prefix)
        collection = meta.collection if meta else None
        if not collection:
            collection = self.object_store_key(name, '.datastore')
            collection = collection.replace('..', '.')
        # return the collection
        try:
            datastore = getattr(self.mongodb, collection)
        except Exception as e:
            raise e
        return PickableCollection(datastore)

    def _apply_mixins(self):
        """
        apply mixins in defaults.OMEGA_STORE_MIXINS
        """
        for mixin in self.defaults.OMEGA_STORE_MIXINS:
            conditional = self._mixins_conditional
            extend_instance(self, mixin,
                            conditional=conditional)

    def _mixins_conditional(self, cls, obj):
        return cls.supports(obj) if hasattr(cls, 'supports') else True

    def register_backends(self):
        """
        register backends in defaults.OMEGA_STORE_BACKENDS
        """
        # enable list modification within loop
        # -- avoid RuntimeError: dictionary changed size during iteration
        backends = list(self.defaults.OMEGA_STORE_BACKENDS.items())
        for kind, backend in backends:
            self.register_backend(kind, backend)

    def register_backend(self, kind, backend, index=-1):
        """
        register a backend class

        :param kind: (str) the backend kind
        :param backend: (class) the backend class
        :param index: (int) the insert position, defaults to -1, which means
          to append

        .. versionchanged:: NEXT
            added index to have more control over backend evaluation by .get_backend_byobj()

        .. versionchanged:: NEXT
            backends can specify cls.KIND_EXT to register additional kinds
        """
        backend_cls = load_class(backend)
        backend_kinds = [backend_cls.KIND] + list(getattr(backend_cls, 'KIND_EXT', []))
        for kind in backend_kinds:
            self.defaults.OMEGA_STORE_BACKENDS[kind] = backend_cls
            if kind not in MDREGISTRY.KINDS:
                pos = len(MDREGISTRY.KINDS) + index if index < 0 else index
                MDREGISTRY.KINDS.insert(pos, kind)
        return self

    def register_mixin(self, mixincls):
        """
        register a mixin class

        :param mixincls: (class) the mixin class
        """
        extend_instance(self, mixincls)
        return self

    def put(self, obj, name, attributes=None, kind=None, replace=False, model_store=None,
            data_store=None, **kwargs):
        """
        Stores an object, store estimators, pipelines, numpy arrays or
        pandas dataframes
        """
        if replace:
            self.drop(name, force=True)
        backend = self.get_backend_byobj(obj, name, attributes=attributes, kind=kind,
                                         model_store=model_store, data_store=data_store,
                                         **kwargs)
        if backend:
            return backend.put(obj, name, attributes=attributes, **kwargs)
        raise TypeError('type %s not supported' % type(obj))

    def drop(self, name, force=False, version=-1, report=False, **kwargs):
        """
        Drop the object

        :param name: The name of the object. If the name is a pattern it will
           be expanded using .list(), and call .drop() on every obj found.
        :param force: If True ignores DoesNotExist exception, defaults to False
            meaning this raises a DoesNotExist exception if the name does not
            exist
        :param report: if True returns a dict name=>status, where status is True
            if deleted, False if not deleted
        :return:    True if object was deleted, False if not.
                    If force is True and the object does not exist it will still return True
        :raises: DoesNotExist if the object does not exist and ```force=False```
        """
        is_pattern = '*' in name
        objs = [name] if not is_pattern else self.list(name)
        results = []
        for name in objs:
            try:
                backend = self.get_backend(name)
                drop = backend.drop if backend else self._drop
                result = drop(name, force=force, version=version, **kwargs)
            except Exception as e:
                result = False
                if not force and not is_pattern:
                    raise
                if force:
                    result = self._drop(name, force=force, version=version)
            results.append((name, result))
        if not objs:
            result = self._drop(name, force=force, version=version)
            results.append((name, result))
        return dict(results) if report else len(results) > 0

    def _drop(self, name, force=False, version=-1, keep_data=False, **kwargs):
        meta = self.metadata(name, version=version)
        if meta is None and not force:
            raise DoesNotExist(name)
        collection = self.collection(name)
        if collection and not keep_data:
            self.mongodb.drop_collection(collection.name)
        if meta:
            if meta.collection and not keep_data:
                self.mongodb.drop_collection(meta.collection)
            if meta and meta.gridfile is not None and not keep_data:
                meta.gridfile.delete()
            self._drop_metadata(name)
            return True
        return False

    def get_backend_bykind(self, kind, model_store=None, data_store=None,
                           **kwargs):
        """
        return the backend by a given object kind

        :param kind: The object kind
        :param model_store: the OmegaStore instance used to store models
        :param data_store: the OmegaStore instance used to store data
        :param kwargs: the kwargs passed to the backend initialization
        :return: the backend
        """
        try:
            backend_cls = load_class(self.defaults.OMEGA_STORE_BACKENDS[kind])
        except KeyError as e:
            raise ValueError('backend {kind} does not exist'.format(**locals()))
        model_store = model_store or self
        data_store = data_store or self
        backend = backend_cls(model_store=model_store,
                              data_store=data_store, **kwargs)
        return backend

    def get_backend(self, name, model_store=None, data_store=None, **kwargs):
        """
        return the backend by a given object name

        :param kind: The object kind
        :param model_store: the OmegaStore instance used to store models
        :param data_store: the OmegaStore instance used to store data
        :param kwargs: the kwargs passed to the backend initialization
        :return: the backend
        """
        meta = self.metadata(name)
        if meta is not None and meta.kind in self.defaults.OMEGA_STORE_BACKENDS:
            return self.get_backend_bykind(meta.kind,
                                           model_store=model_store,
                                           data_store=data_store,
                                           **kwargs)
        return None

    def help(self, name_or_obj=None, kind=None, raw=False, display=None, renderer=None):
        """ get help for an object by looking up its backend and calling help() on it

        Retrieves the object's metadata and looks up its corresponding backend. If the
        metadata.attributes['docs'] is a string it will display this as the help() contents.
        If the string starts with 'http://' or 'https://' it will open the web page.

        Args:
            name_or_obj (str|obj): the name or actual object to get help for
            kind (str): optional, if specified forces retrieval of backend for the given kind
            raw (bool): optional, if True forces help to be the backend type of the object.
                If False returns the attributes[docs] on the object's metadata, if available.
                Defaults to False
            display (fn): optional, callable for interactive display, defaults to help in
                if sys.flags.interactive is True, else uses pydoc.render_doc with plaintext
            renderer (fn): optional, the renderer= argument for pydoc.render_doc to use if
                sys.flags.interactive is False and display is not provided

        Returns:
            * help(obj) if python is in interactive mode
            * text(str) if python is in not interactive mode
        """
        import sys
        import pydoc
        interactive = bool(display) if display is not None else sys.flags.interactive
        display = display or help
        renderer = renderer or pydoc.plaintext
        obj = self._resolve_help_backend(name_or_obj=name_or_obj, kind=kind, raw=raw)
        if any(str(obj.__doc__).startswith(v) for v in ('https://', 'http://')):
            obj = obj.__doc__
            if interactive and display is help:
                import webbrowser
                display = webbrowser.open
        return display(obj) if interactive else pydoc.render_doc(obj, renderer=renderer)

    def _resolve_help_backend(self, name_or_obj=None, kind=None, raw=False):
        # helper so we can test help
        meta = self.metadata(name_or_obj) if name_or_obj else None
        if kind:
            backend = self.get_backend_bykind(kind)
        else:
            backend = self.get_backend(name_or_obj) or self.get_backend_byobj(name_or_obj)
        if backend is None:
            backend = self.get_backend_bykind('core.object', model_store=self, data_store=self)
        if not raw and meta is not None and 'docs' in meta.attributes:
            def UserDocumentation():
                pass

            basedoc = backend.__doc__ or ''
            UserDocumentation.__doc__ = (basedoc +
                                         meta.attributes['docs'])
            backend = UserDocumentation
        return backend

    def get_backend_byobj(self, obj, name=None, kind=None, attributes=None,
                          model_store=None, data_store=None, **kwargs):
        """
        return the matching backend for the given obj

        Returns:
            the first backend that supports the given parameters or None

        .. versionchanged:: NEXT
            backends are tested in order of MDREGISTRY.KINDS, see .register_backend()
        """
        model_store = model_store or self
        data_store = data_store or self
        meta = self.metadata(name) if name else None
        kind = kind or (meta.kind if meta is not None else None)
        backend = None
        if kind:
            if kind in self.defaults.OMEGA_STORE_BACKENDS:
                backend = self.get_backend_bykind(kind, data_store=data_store, model_store=model_store)
                if not backend.supports(obj, name, attributes=attributes,
                                        data_store=data_store,
                                        model_store=model_store,
                                        meta=meta,
                                        kind=kind, **kwargs):
                    objtype = str(type(obj))
                    warnings.warn('Backend {kind} does not support {objtype}'.format(**locals()))
            else:
                # revert to core backend
                backend = self.get_backend_bykind('core.object', model_store=model_store, data_store=data_store)
        else:
            # sort by order in MDREGISTRY.KINDS, only using kinds that actually have a registered backend
            sorted_backends = (k for k in MDREGISTRY.KINDS if k in self.defaults.OMEGA_STORE_BACKENDS)
            for backend_kind in sorted_backends:
                backend = self.get_backend_bykind(backend_kind, data_store=data_store, model_store=model_store)
                if backend.supports(obj, name, attributes=attributes,
                                    data_store=data_store, model_store=model_store,
                                    **kwargs):
                    break
            else:
                backend = None
        return backend

    def getl(self, *args, **kwargs):
        """ return a lazy MDataFrame for a given object

        Same as .get, but returns a MDataFrame

        """
        return self.get(*args, lazy=True, **kwargs)

    def get(self, name, version=-1, force_python=False,
            kind=None, model_store=None, data_store=None, **kwargs):
        """
        Retrieve an object

        :param name: The name of the object
        :param version: Version of the stored object (not supported)
        :param force_python: Return as a python object
        :param kwargs: kwargs depending on object kind
        :return: an object, estimator, pipelines, data array or pandas dataframe
            previously stored with put()
        """
        meta = self.metadata(name, version=version)
        if meta is None:
            return None
        if not force_python:
            backend = (self.get_backend(name, model_store=model_store,
                                        data_store=data_store, **kwargs)
                       if not kind else self.get_backend_bykind(kind, model_store=model_store,
                                                                data_store=data_store, **kwargs))
            if backend is not None:
                # FIXME: some backends need to get model_store, data_store, but fails tests
                return backend.get(name, **kwargs)  # model_store=model_store, data_store=data_store, **kwargs)
        # catch-call to CoreObjectsBackend or force python
        # -- keeping the same behavior until version 0.17, handling all other KINDs
        core_backend = self.get_backend_bykind('core.object', model_store=model_store, data_store=data_store, **kwargs)
        if force_python:
            return core_backend.get_object_as_python(meta, version=version)
        return core_backend.get(name, version=version, force_python=force_python, **kwargs)

    def __iter__(self):
        for f in self.list(include_temp=True):
            yield f

    @property
    def buckets(self):
        return ['default' if b == self.defaults.OMEGA_MONGO_COLLECTION else b
                for b in self._Metadata.objects.distinct('bucket')]

    def list(self, pattern=None, regexp=None, kind=None, raw=False, hidden=None,
             include_temp=False, bucket=None, prefix=None, filter=None):
        """
        List all files in store

        specify pattern as a unix pattern (e.g. :code:`models/*`,
        or specify regexp)

        :param pattern: the unix file pattern or None for all
        :param regexp: the regexp. takes precedence over pattern
        :param raw: if True return the meta data objects
        :param filter: specify additional filter criteria, optional
        :return: List of files in store

        """
        regex = lambda pattern: bson.regex.Regex(f'{pattern}')
        db = self.mongodb
        searchkeys = dict(bucket=bucket or self.bucket,
                          prefix=prefix or self.prefix)
        q_excludes = Q()
        if regexp:
            searchkeys['name'] = regex(regexp)
        elif pattern:
            re_pattern = pattern.replace('*', '.*').replace('/', r'\/')
            searchkeys['name'] = regex(f'^{re_pattern}$')
        if not include_temp:
            q_excludes &= Q(name__not__startswith='_')
            q_excludes &= Q(name__not=regex(r'(.{1,*}\/?_.*)'))
        if not hidden:
            q_excludes &= Q(name__not__startswith='.')
        if kind or self.force_kind:
            kind = kind or self.force_kind
            if isinstance(kind, (tuple, list)):
                searchkeys.update(kind__in=kind)
            else:
                searchkeys.update(kind=kind)
        if filter:
            searchkeys.update(filter)
        q_search = Q(**searchkeys) & q_excludes
        files = self._Metadata.objects.no_cache()(q_search)
        return [f if raw else str(f.name).replace('.omm', '') for f in files]

    def exists(self, name, hidden=False):
        """ check if object exists

        Args:
            name (str): name of object
            hidden (bool): if True, include hidden files, defaults to
              False, unless name starts with '.'

        Returns:
            bool, True if object exists

        .. versionchanged:: 0.16.4
            hidden defaults to True if name starts with '.'
        """
        hidden = True if name.startswith('.') else hidden
        return name in self.list(name, hidden=hidden)

    def object_store_key(self, name, ext, hashed=None):
        """
        Returns the store key

        Unless you write a mixin or a backend you should not use this method

        :param name: The name of object
        :param ext: The extension of the filename
        :param hashed: hash the key to support arbitrary name length, defaults
           to defaults.OMEGA_STORE_HASHEDNAMES, True by default since 0.13.7

        :return: A filename with relative bucket, prefix and name
        """
        # byte string
        _u8 = lambda t: t.encode('UTF-8', 'replace') if isinstance(t, str) else t
        key = self._get_obj_store_key(name, ext)
        hashed = hashed if hashed is not None else self.defaults.OMEGA_STORE_HASHEDNAMES
        if hashed:
            from hashlib import sha1
            # SEC: CWE-916
            # - status: wontfix
            # - reason: hashcode is used purely for name resolution, not a security function
            hasher = sha1()
            hasher.update(_u8(key))
            key = hasher.hexdigest()
        return key

    def _get_obj_store_key(self, name, ext, prefix=None, bucket=None):
        # backwards compatilibity implementation of object_store_key()
        name = '%s.%s' % (name, ext) if not name.endswith(ext) else name
        filename = '{bucket}.{prefix}.{name}'.format(
            bucket=bucket or self.bucket,
            prefix=prefix or self.prefix,
            name=name,
            ext=ext).replace('/', '_').replace('..', '.')
        return filename

    def _ensure_fs_collection(self):
        # ensure backwards-compatible gridfs access
        if self.defaults.OMEGA_BUCKET_FS_LEGACY:
            # prior to 0.13.2 a single gridfs instance was used, always equal to the default collection
            return self.defaults.OMEGA_MONGO_COLLECTION
        if self.bucket == self.defaults.OMEGA_MONGO_COLLECTION:
            # from 0.13.2 onwards, only the default bucket is equal to the default collection
            # backwards compatibility for existing installations
            return self.bucket
        # since 0.13.2, all buckets other than the default use a qualified collection name to
        # effectively separate files in different buckets, enabling finer-grade access control
        # and avoiding name collisions from different buckets
        return '{}_{}'.format(self.defaults.OMEGA_MONGO_COLLECTION, self.bucket)

    def _ensure_fs_index(self, fs):
        # make sure we have proper chunks and file indicies. this should be created on first write, but sometimes is not
        # see https://docs.mongodb.com/manual/core/gridfs/#gridfs-indexes
        # pymongo since 4.1 no longer has fs._GridFS
        chunks_collection = fs._GridFS__chunks if hasattr(fs, '_GridFS__chunks') else fs._chunks
        files_collection = fs._GridFS__files if hasattr(fs, '_GridFS__files') else fs._files
        ensure_index(chunks_collection, {'files_id': 1, 'n': 1}, unique=True)
        ensure_index(files_collection, {'filename': 1, 'uploadDate': 1})

    def sign(self, filter):
        return signature(filter)

    @staticmethod
    def _cleanup(objrepr, tmppath=None):
        # cleanup any temporary files on exit
        # -- this is called as a finalizer (weakref.finalize)
        try:
            logger.debug(f'finalizing {objrepr}: cleaning up temporary files in {tmppath}')
            shutil.rmtree(tmppath, ignore_errors=True)
        except Exception as e:
            logger.error(f'finalizing {objrepr}: error occured as {e}')
