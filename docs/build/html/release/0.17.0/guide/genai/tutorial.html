

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorial for Generative AI &mdash; omega-ml 0.17.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=7ca4e891" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=0c3dd48d"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Classic ML" href="../classicml/index.html" />
    <link rel="prev" title="Concepts in omega-ml Generative AI" href="concepts.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            omega-ml
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../quickstart/index.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">User Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../intro/index.html">Getting started</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Generative AI</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="concepts.html">Concepts in omega-ml Generative AI</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Tutorial for Generative AI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#deploy-a-generative-ai-model">Deploy a generative AI model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#document-storage">Document storage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#building-a-rag-pipeline">Building a RAG pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="#adding-tools">Adding tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="#adding-custom-pipeline-actions">Adding custom pipeline actions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#serving-a-model">Serving a model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tracking-model-interactions">Tracking model interactions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-a-third-party-llm-framework">Using a third-party LLM framework</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../classicml/index.html">Classic ML</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pipelines/index.html">Task Automation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../clusters/index.html">Deployment and Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cli/index.html">Command-line interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/index.html">Advanced features</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../admin/index.html">Deploying omega-ml</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../devguide/index.html">Extending omega-ml</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/index.html">Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../support.html">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/index.html">Changes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">omega-ml</a>
      </nav>

      <div class="wy-nav-content">


        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">User Guide</a></li>
          <li class="breadcrumb-item"><a href="index.html">Generative AI</a></li>
      <li class="breadcrumb-item active">Tutorial for Generative AI</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/guide/genai/tutorial.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tutorial-for-generative-ai">
<h1>Tutorial for Generative AI<a class="headerlink" href="#tutorial-for-generative-ai" title="Link to this heading">¶</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#deploy-a-generative-ai-model" id="id1">Deploy a generative AI model</a></p></li>
<li><p><a class="reference internal" href="#document-storage" id="id2">Document storage</a></p></li>
<li><p><a class="reference internal" href="#building-a-rag-pipeline" id="id3">Building a RAG pipeline</a></p></li>
<li><p><a class="reference internal" href="#adding-tools" id="id4">Adding tools</a></p></li>
<li><p><a class="reference internal" href="#adding-custom-pipeline-actions" id="id5">Adding custom pipeline actions</a></p></li>
<li><p><a class="reference internal" href="#serving-a-model" id="id6">Serving a model</a></p></li>
<li><p><a class="reference internal" href="#tracking-model-interactions" id="id7">Tracking model interactions</a></p></li>
<li><p><a class="reference internal" href="#using-a-third-party-llm-framework" id="id8">Using a third-party LLM framework</a></p></li>
</ul>
</nav>
<section id="deploy-a-generative-ai-model">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Deploy a generative AI model</a><a class="headerlink" href="#deploy-a-generative-ai-model" title="Link to this heading">¶</a></h2>
<p>To register a model hosted by a third-party provider that is compatible with OpenAI’s API or SDK,
use the following syntax:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">PROVIDER_APIKEY</span> <span class="o">=</span> <span class="s1">&#39;your-api-key&#39;</span>
<span class="n">model_url</span> <span class="o">=</span> <span class="p">(</span><span class="sa">f</span><span class="s1">&#39;openai+https://</span><span class="si">{</span><span class="n">PROVIDER_APIKEY</span><span class="si">}</span><span class="s1">@openrouter.ai/api/v1&#39;</span>
              <span class="s1">&#39;;model=google/gemini-2.0-flash-lite-preview-02-05:free&#39;</span><span class="p">)</span>
<span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">model_url</span><span class="p">,</span> <span class="s1">&#39;llm&#39;</span><span class="p">)</span>
<span class="o">=&gt;</span>
<span class="o">&lt;</span><span class="n">Metadata</span><span class="p">:</span> <span class="n">Metadata</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span><span class="n">bucket</span><span class="o">=</span><span class="n">omegaml</span><span class="p">,</span><span class="n">prefix</span><span class="o">=</span><span class="n">models</span><span class="o">/</span><span class="p">,</span><span class="n">kind</span><span class="o">=</span><span class="n">genai</span><span class="o">.</span><span class="n">text</span><span class="p">,</span><span class="n">created</span><span class="o">=</span><span class="mi">2025</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">21</span> <span class="mi">19</span><span class="p">:</span><span class="mi">25</span><span class="p">:</span><span class="mf">29.325000</span><span class="p">)</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>This model is now available for completions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">complete</span><span class="p">(</span><span class="s1">&#39;hello, who are you?&#39;</span><span class="p">)</span>
<span class="o">=&gt;</span>
<span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span>
 <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Hello! I am a large language model, trained by Google.&#39;</span><span class="p">,</span>
 <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;e99121a1050a463abaf2c913e99ff5ba&#39;</span><span class="p">}</span>
</pre></div>
</div>
<p>We can also serve the model for access by thid-party applications. This will start a REST API server
that can be accessed by third-party applications.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>om<span class="w"> </span>runtime<span class="w"> </span>serve
$<span class="w"> </span>!curl<span class="w"> </span>-X<span class="w"> </span>PUT<span class="w"> </span>http://localhost:8000/api/v1/model/llm/complete<span class="w"> </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{ &quot;prompt&quot;: &quot;hello&quot;}&#39;</span>
<span class="o">{</span><span class="s2">&quot;model&quot;</span>:<span class="w"> </span><span class="s2">&quot;llm&quot;</span>,<span class="w"> </span><span class="s2">&quot;result&quot;</span>:<span class="w"> </span><span class="o">{</span><span class="s2">&quot;role&quot;</span>:<span class="w"> </span><span class="s2">&quot;assistant&quot;</span>,<span class="w"> </span><span class="s2">&quot;content&quot;</span>:<span class="w"> </span><span class="s2">&quot;Hello there! How can I help you today? \ud83d\ude0a&quot;</span>,
<span class="w"> </span><span class="s2">&quot;conversation_id&quot;</span>:<span class="w"> </span><span class="s2">&quot;60056750ec39433f9533e7cbf60c65cc&quot;</span><span class="o">}</span>,<span class="w"> </span><span class="s2">&quot;resource_uri&quot;</span>:<span class="w"> </span><span class="s2">&quot;llm&quot;</span><span class="o">}</span>
</pre></div>
</div>
</section>
<section id="document-storage">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Document storage</a><a class="headerlink" href="#document-storage" title="Link to this heading">¶</a></h2>
<p>In support of Retrieval Augmented Generation (RAG), omega-ml provides a built-in document storage, using document
embeddings and PostgreSQL as the vector DB (with the pgvector extension). To use this we first need an embedding
model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">apikey</span> <span class="o">=</span> <span class="s1">&#39;apikey&#39;</span>
<span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;openai+https://</span><span class="si">{</span><span class="n">apikey</span><span class="si">}</span><span class="s1">@api.jina.ai/v1/;model=jina-embeddings-v3&#39;</span><span class="p">,</span>  <span class="s1">&#39;jina&#39;</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">=&gt;</span>
<span class="o">&lt;</span><span class="n">Metadata</span><span class="p">:</span> <span class="n">Metadata</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">jina</span><span class="p">,</span><span class="n">bucket</span><span class="o">=</span><span class="n">omegaml</span><span class="p">,</span><span class="n">prefix</span><span class="o">=</span><span class="n">models</span><span class="o">/</span><span class="p">,</span><span class="n">kind</span><span class="o">=</span><span class="n">genai</span><span class="o">.</span><span class="n">text</span><span class="p">,</span><span class="n">created</span><span class="o">=</span><span class="mi">2025</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">03</span> <span class="mi">14</span><span class="p">:</span><span class="mi">33</span><span class="p">:</span><span class="mf">09.643173</span><span class="p">)</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>To store actual documents, we can create a document store:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">om</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="s1">&#39;pgvector://postgres:test@localhost:5432/postgres&#39;</span><span class="p">,</span> <span class="s1">&#39;documents&#39;</span><span class="p">,</span>
                <span class="n">embedding_model</span><span class="o">=</span><span class="s1">&#39;jina&#39;</span><span class="p">,</span> <span class="n">model_store</span><span class="o">=</span><span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">=&gt;</span>
<span class="o">&lt;</span><span class="n">Metadata</span><span class="p">:</span> <span class="n">Metadata</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">documents</span><span class="p">,</span><span class="n">bucket</span><span class="o">=</span><span class="n">omegaml</span><span class="p">,</span><span class="n">prefix</span><span class="o">=</span><span class="n">data</span><span class="o">/</span><span class="p">,</span><span class="n">kind</span><span class="o">=</span><span class="n">pgvector</span><span class="o">.</span><span class="n">conx</span><span class="p">,</span><span class="n">created</span><span class="o">=</span><span class="mi">2025</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">03</span> <span class="mi">14</span><span class="p">:</span><span class="mi">34</span><span class="p">:</span><span class="mf">23.606619</span><span class="p">)</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Now we can insert documents into the store. The documents are automatically chunked and embeddings created
using the embedding model of the store.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">Path</span><span class="p">(</span><span class="s1">&#39;/path/to/documents&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;*.pdf&#39;</span><span class="p">):</span>
    <span class="n">om</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="s1">&#39;documents&#39;</span><span class="p">,</span> <span class="n">model_store</span><span class="o">=</span><span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="p">)</span>
</pre></div>
</div>
<p>Once the documents are stored, we can query the document store. The results are returned as a
list of documents, sorted by relevance. The first document has the highest relevance score.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">om</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;documents&#39;</span><span class="p">,</span> <span class="n">query</span><span class="o">=</span><span class="s1">&#39;hello world&#39;</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="o">=&gt;</span>
<span class="p">[</span><span class="n">Document</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s1">&#39;Hello world! This is a test document.&#39;</span><span class="p">,</span> <span class="n">score</span><span class="o">=</span><span class="mf">0.9</span><span class="p">),</span>
 <span class="n">Document</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s1">&#39;Another test document.&#39;</span><span class="p">,</span> <span class="n">score</span><span class="o">=</span><span class="mf">0.8</span><span class="p">),</span>
 <span class="n">Document</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s1">&#39;Yet another test document.&#39;</span><span class="p">,</span> <span class="n">score</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)]</span>
</pre></div>
</div>
</section>
<section id="building-a-rag-pipeline">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Building a RAG pipeline</a><a class="headerlink" href="#building-a-rag-pipeline" title="Link to this heading">¶</a></h2>
<p>To build a RAG pipeline, we attach a document store to the model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;openai+https://</span><span class="si">{</span><span class="n">apikey</span><span class="si">}</span><span class="s1">@api.jina.ai/v1/;model=jina-embeddings-v3&#39;</span><span class="p">,</span>  <span class="s1">&#39;jina&#39;</span><span class="p">,</span>
              <span class="n">documents</span><span class="o">=</span><span class="s1">&#39;documents&#39;</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>When we ask for a completion, we can add the context in the prompt as <code class="code docutils literal notranslate"><span class="pre">{context}</span></code>.
The context is automatically retrieved from the document store, using the prompt as the query. The
top document is used as the context.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;llm&#39;</span><span class="p">,</span> <span class="n">data_store</span><span class="o">=</span><span class="n">om</span><span class="o">.</span><span class="n">datasets</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">complete</span><span class="p">(</span><span class="s1">&#39;what is the sum of the invoice? Just say SUM=&lt;sum&gt;. context: </span><span class="si">{context}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span>
 <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;SUM=15.00&#39;</span><span class="p">,</span>
 <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;024f8b43dcb74211a836a7042d067c8f&#39;</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="adding-tools">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Adding tools</a><a class="headerlink" href="#adding-tools" title="Link to this heading">¶</a></h2>
<p>Tools are functions that can be called by a model. They are used to extend the capabilities of the model
beyond text generation. For example, we can add a tool that calculates the sum of a list of numbers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sum_numbers</span><span class="p">(</span><span class="n">numbers</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">numbers</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)))</span> <span class="c1"># the model passes as string</span>

<span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">sum_numbers</span><span class="p">,</span> <span class="s1">&#39;tools/sum_numbers&#39;</span><span class="p">)</span>

<span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;openai+https://</span><span class="si">{</span><span class="n">apikey</span><span class="si">}</span><span class="s1">@openrouter.ai/api/v1;model=google/gemini-2.0-flash-exp:free&#39;</span><span class="p">,</span>
              <span class="s1">&#39;llm&#39;</span><span class="p">,</span> <span class="n">documents</span><span class="o">=</span><span class="s1">&#39;documents&#39;</span><span class="p">,</span> <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sum_numbers&#39;</span><span class="p">],</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;llm&#39;</span><span class="p">,</span> <span class="n">data_store</span><span class="o">=</span><span class="n">om</span><span class="o">.</span><span class="n">datasets</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">complete</span><span class="p">(</span><span class="s1">&#39;What is the sum of 7, 9, 24?&#39;</span><span class="p">)</span>
<span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span>
 <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;The sum of 7, 9, and 24 is 40.</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span>
 <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;33738231f39047dcb886500143cebf8a&#39;</span><span class="p">,</span>
 <span class="s1">&#39;intermediate_results&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;tool_calls&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="s1">&#39;id&#39;</span><span class="p">:</span> <span class="s1">&#39;tool_0_sum_numbers&#39;</span><span class="p">,</span>
    <span class="s1">&#39;function&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;arguments&#39;</span><span class="p">:</span> <span class="s1">&#39;{&quot;numbers&quot;:&quot;7,9,24&quot;}&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;sum_numbers&#39;</span><span class="p">},</span>
    <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;function&#39;</span><span class="p">,</span>
    <span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}],</span>
  <span class="s1">&#39;tool_prompts&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;tool&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tool_call_id&#39;</span><span class="p">:</span> <span class="s1">&#39;tool_0_sum_numbers&#39;</span><span class="p">,</span>
    <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;40&#39;</span><span class="p">}],</span>
  <span class="s1">&#39;tool_results&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span>
    <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
    <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;33738231f39047dcb886500143cebf8a&#39;</span><span class="p">}]}}</span>
</pre></div>
</div>
</section>
<section id="adding-custom-pipeline-actions">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Adding custom pipeline actions</a><a class="headerlink" href="#adding-custom-pipeline-actions" title="Link to this heading">¶</a></h2>
<p>In omega-ml, a generative model is in effect a pipeline of multiple steps. For each step, the model can
call custom code to adjust the processing of each step. For example, we can add custom code that checks
the user’s prompt before processing, or add guardrails to check the output of the model, and if necessary,
modify the output or return any other response.</p>
<p>A pipeline is a specific type of virtual function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">omegaml.backends.genai.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">virtual_genai</span>

<span class="nd">@virtual_genai</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pipeline</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;calling method=</span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   args=</span><span class="si">{</span><span class="n">args</span><span class="si">}</span><span class="s2">, kwargs=</span><span class="si">{</span><span class="n">kwargs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s1">&#39;pipeline&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Add the pipeline to the model</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;openai+https://</span><span class="si">{</span><span class="n">APIKEY</span><span class="si">}</span><span class="s1">@openrouter.ai/api/v1;model=google/gemini-2.0-flash-exp:free&#39;</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">model_url</span><span class="p">,</span> <span class="s1">&#39;llm&#39;</span><span class="p">,</span> <span class="n">pipeline</span><span class="o">=</span><span class="s1">&#39;pipeline&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>When we call the model, it will call the pipeline function for each step. For every step we can
add custom code to process the input and output of the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;llm&#39;</span><span class="p">,</span> <span class="n">data_store</span><span class="o">=</span><span class="n">om</span><span class="o">.</span><span class="n">datasets</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">complete</span><span class="p">(</span><span class="s1">&#39;hello world&#39;</span><span class="p">)</span>
<span class="o">=&gt;</span>
<span class="n">calling</span> <span class="n">method</span><span class="o">=</span><span class="n">template</span>
    <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;meta&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;store&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;tracking&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;prompt_message&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;hello&#39;</span><span class="p">,</span> <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;aae49e95404f4a4f99245e9237256017&#39;</span><span class="p">},</span> <span class="s1">&#39;messages&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="p">,</span> <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;aae49e95404f4a4f99245e9237256017&#39;</span><span class="p">}],</span> <span class="s1">&#39;template&#39;</span><span class="p">:</span> <span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="p">,</span> <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;aae49e95404f4a4f99245e9237256017&#39;</span><span class="p">}</span>
<span class="n">calling</span> <span class="n">method</span><span class="o">=</span><span class="n">prepare</span>
    <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;meta&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;store&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;tracking&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;prompt_message&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;hello&#39;</span><span class="p">,</span> <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;aae49e95404f4a4f99245e9237256017&#39;</span><span class="p">},</span> <span class="s1">&#39;messages&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="p">,</span> <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;aae49e95404f4a4f99245e9237256017&#39;</span><span class="p">}],</span> <span class="s1">&#39;template&#39;</span><span class="p">:</span> <span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="p">,</span> <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;aae49e95404f4a4f99245e9237256017&#39;</span><span class="p">}</span>
<span class="n">calling</span> <span class="n">method</span><span class="o">=</span><span class="n">process</span>
    <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;meta&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;store&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;tracking&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;response_message&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Hello! How can I help you today? 😊&#39;</span><span class="p">,</span> <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;aae49e95404f4a4f99245e9237256017&#39;</span><span class="p">},</span> <span class="s1">&#39;prompt_message&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;hello&#39;</span><span class="p">,</span> <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;aae49e95404f4a4f99245e9237256017&#39;</span><span class="p">},</span> <span class="s1">&#39;messages&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="p">,</span> <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;aae49e95404f4a4f99245e9237256017&#39;</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;hello&#39;</span><span class="p">,</span> <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;aae49e95404f4a4f99245e9237256017&#39;</span><span class="p">}],</span> <span class="s1">&#39;template&#39;</span><span class="p">:</span> <span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="p">,</span> <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;aae49e95404f4a4f99245e9237256017&#39;</span><span class="p">}</span>
<span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span>
 <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Hello! How can I help you today? 😊&#39;</span><span class="p">,</span>
 <span class="s1">&#39;conversation_id&#39;</span><span class="p">:</span> <span class="s1">&#39;aae49e95404f4a4f99245e9237256017&#39;</span><span class="p">}</span>
</pre></div>
</div>
<p>The steps of the pipeline are:</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>template</strong> - the template is used to generate the prompt for the model. The template is</dt><dd><p>generated from the model’s metadata. This should return the template to use.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>prepare</strong> - the prepare step is used to prepare the input messages for the model. The input</dt><dd><p>messages are generated from the template and the prompt message. This should return the
list of messages to use.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>process</strong> - the process step is used to process the output of the model.</dt><dd><p>This should return the final output of the model.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>toolcall</strong> - the toolcall step is used to process the output of a tool. The output can be</dt><dd><p>modified by the pipeline. This should return the messages to be sent back to the model.
Semantically this is the same as the <strong>prepare</strong> step.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>toolresult</strong> - the toolresult step is used to process the response of the model to a tools’ result.</dt><dd><p>This should return the output of the model. Semantically this is the same as the <strong>process</strong> step.</p>
</dd>
</dl>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are currently no steps for the RAG part of the pipeline. However, you can use the <strong>prepare</strong>
message to process the input messages and modify the context, or add a custom context.</p>
</div>
<p>The function signature is the same for all steps:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">pipeline</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">template</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prompt_message</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">messages</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">response_message</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">conversation_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        *args: positional arguments</span>
<span class="sd">        method (str): the name of the pipeline step</span>
<span class="sd">        template (str): the template to use</span>
<span class="sd">        prompt_message (str): the prompt message</span>
<span class="sd">        messages (list): the list of messages, this is in the format of the model provider,</span>
<span class="sd">          e.g. [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;hello world&#39;}, ...]</span>
<span class="sd">        response_message (dict): the response message, this is the format of the model provider,</span>
<span class="sd">          e.g. {&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;hello world&#39;}</span>
<span class="sd">        conversation_id (str): the conversation id</span>
<span class="sd">        **kwargs: keyword arguments</span>

<span class="sd">    Returns:</span>
<span class="sd">        * None: to continue the pipeline without changes</span>
<span class="sd">        * for method=template: the template string to use</span>
<span class="sd">        * for method=prepare: the list of messages to use, each message must of</span>
<span class="sd">            format {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;hello world&#39;}</span>
<span class="sd">        * for method=process: the response message to use as a dict of format</span>
<span class="sd">            {&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;hello world&#39;}</span>
<span class="sd">        * for method=toolcall: the list of messages to use, each message must of</span>
<span class="sd">            format {&#39;role&#39;: &#39;tool&#39;, &#39;content&#39;: &#39;hello world&#39;}</span>
<span class="sd">        * for method=toolresult: the response message to use as a dict of format</span>
<span class="sd">            {&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;hello world&#39;}</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>
</div>
</section>
<section id="serving-a-model">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Serving a model</a><a class="headerlink" href="#serving-a-model" title="Link to this heading">¶</a></h2>
<p>To serve a model, we start the integrated REST API server and call the model using curl.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>om<span class="w"> </span>runtime<span class="w"> </span>serve
$<span class="w"> </span>curl<span class="w"> </span>-X<span class="w"> </span>PUT<span class="w"> </span>http://localhost:8000/api/v1/model/llm/complete<span class="w"> </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{ &quot;prompt&quot;: &quot;hello again!&quot;}&#39;</span>
</pre></div>
</div>
</section>
<section id="tracking-model-interactions">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Tracking model interactions</a><a class="headerlink" href="#tracking-model-interactions" title="Link to this heading">¶</a></h2>
<p>To track a model’s inputs and outputs, we can use the <code class="code docutils literal notranslate"><span class="pre">track()</span></code> method. This will
automatically capture all calls to the model via omega-ml’s runtime or via the REST API.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span> <span class="o">=</span> <span class="n">om</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">experiment</span><span class="p">(</span><span class="s1">&#39;myexp&#39;</span><span class="p">)</span>
<span class="n">exp</span><span class="o">.</span><span class="n">track</span><span class="p">(</span><span class="s1">&#39;llm&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This automatically tracks all input and output of the model. By using omega-ml’s distributed runtime architecture,
this works the same locally as with a scaled-up distributed environment like Kubernetes.</p>
<p>To access the tracking data, we can</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span> <span class="o">=</span> <span class="n">om</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">experiment</span><span class="p">(</span><span class="s1">&#39;myexp&#39;</span><span class="p">)</span>
<span class="n">exp</span><span class="o">.</span><span class="n">data</span><span class="p">()</span>
<span class="o">=&gt;</span>
<span class="o">&lt;</span><span class="n">DataFrame</span><span class="o">&gt;</span>
</pre></div>
</div>
</section>
<section id="using-a-third-party-llm-framework">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Using a third-party LLM framework</a><a class="headerlink" href="#using-a-third-party-llm-framework" title="Link to this heading">¶</a></h2>
<p>In some cases the omega-ml RAG pipeline or document storage may not be sufficient for your needs.
In this case you can use any third-party framework, such as LangChain, Haystack or LlamaIndex.</p>
<p>For this purpose, implement a custom model as a virtualobj:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># define your custom pipeline</span>
<span class="nd">@virtual_genai</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mypipeline</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2"> args=</span><span class="si">{</span><span class="n">args</span><span class="si">}</span><span class="s2"> kwargs=</span><span class="si">{</span><span class="n">kwargs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># trace message for testing</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">langchain</span>
    <span class="n">results</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># store the custom pipeline</span>
<span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">mypipeline</span><span class="p">,</span> <span class="s1">&#39;mypipeline&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">om</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;mypipeline&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This model can now be called like any other generative model. It supports the <code class="code docutils literal notranslate"><span class="pre">chat()</span></code>, <code class="code docutils literal notranslate"><span class="pre">complete()</span></code> and
<code class="code docutils literal notranslate"><span class="pre">embed()</span></code> methods. In this case you should provide the code of the pipeline in full. Please refer
to the documentation of the respective framework for details.</p>
<p>The function shall return the result as the final message to be sent back to the client. The result should be in the
same response format of your model provider, typically in OpenAI format.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="s2">&quot;hello world&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">complete</span><span class="p">(</span><span class="s2">&quot;hello world&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;hello world&quot;</span><span class="p">)</span>
<span class="o">=&gt;</span>
<span class="n">complete</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;foo&#39;</span><span class="p">,)</span> <span class="n">kwargs</span><span class="o">=</span><span class="p">{}</span>
<span class="n">chat</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;foo&#39;</span><span class="p">,)</span> <span class="n">kwargs</span><span class="o">=</span><span class="p">{}</span>
<span class="n">embed</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;foo&#39;</span><span class="p">,)</span> <span class="n">kwargs</span><span class="o">=</span><span class="p">{}</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="concepts.html" class="btn btn-neutral float-left" title="Concepts in omega-ml Generative AI" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../classicml/index.html" class="btn btn-neutral float-right" title="Classic ML" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025 (c) omegaml.io by one2seven GmbH.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>

      </div>
    </section>
  </div>
  <!-- version plugin for rtd template without sidebar -->
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: release/0.17.0
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../../../0.11.3/index.html" class="version-ref-0.11.3">0.11.3</a></dd>
      <dd><a href="../../../../0.11.4/index.html" class="version-ref-0.11.4">0.11.4</a></dd>
      <dd><a href="../../../../0.12.0/index.html" class="version-ref-0.12.0">0.12.0</a></dd>
      <dd><a href="../../../../0.13.0/index.html" class="version-ref-0.13.0">0.13.0</a></dd>
      <dd><a href="../../../../0.13.2/index.html" class="version-ref-0.13.2">0.13.2</a></dd>
      <dd><a href="../../../../0.13.4/index.html" class="version-ref-0.13.4">0.13.4</a></dd>
      <dd><a href="../../../../0.13.5/index.html" class="version-ref-0.13.5">0.13.5</a></dd>
      <dd><a href="../../../../0.13.6/index.html" class="version-ref-0.13.6">0.13.6</a></dd>
      <dd><a href="../../../../0.13.7/index.html" class="version-ref-0.13.7">0.13.7</a></dd>
      <dd><a href="../../../../0.14.0/index.html" class="version-ref-0.14.0">0.14.0</a></dd>
      <dd><a href="../../../../0.15.1/index.html" class="version-ref-0.15.1">0.15.1</a></dd>
      <dd><a href="../../../../0.15.2/index.html" class="version-ref-0.15.2">0.15.2</a></dd>
      <dd><a href="../../../../latest/guide/genai/tutorial.html" class="version-ref-latest">latest</a></dd>
      <dd><a href="../../../0.15.3/index.html" class="version-ref-release/0.15.3">0.15.3</a></dd>
      <dd><a href="../../../0.15.5/index.html" class="version-ref-release/0.15.5">0.15.5</a></dd>
      <dd><a href="../../../0.16.0/index.html" class="version-ref-release/0.16.0">0.16.0</a></dd>
      <dd><a href="../../../0.16.1/index.html" class="version-ref-release/0.16.1">0.16.1</a></dd>
      <dd><a href="../../../0.16.2/index.html" class="version-ref-release/0.16.2">0.16.2</a></dd>
      <dd><a href="../../../0.16.3/index.html" class="version-ref-release/0.16.3">0.16.3</a></dd>
      <dd><a href="../../../0.16.4/index.html" class="version-ref-release/0.16.4">0.16.4</a></dd>
      <dd><a href="tutorial.html" class="version-ref-release/0.17.0">0.17.0</a></dd>
      <dd><a href="../../../0.4/index.html" class="version-ref-release/0.4">0.4</a></dd>
      <dd><a href="../../../0.5/index.html" class="version-ref-release/0.5">0.5</a></dd>
      <dd><a href="../../../0.9/index.html" class="version-ref-release/0.9">0.9</a></dd>
      <dd><a href="../../../../stable/guide/genai/tutorial.html" class="version-ref-stable">stable</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../../../../master/guide/genai/tutorial.html">master</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>