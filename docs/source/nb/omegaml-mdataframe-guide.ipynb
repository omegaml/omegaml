{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with MDataFrame\n",
    "\n",
    "A short guide\n",
    "\n",
    "### What is MDataFrame?\n",
    "\n",
    "* `MDataFrame` provides a Pandas-like API to omega-ml's analytics storage (backed by MongoDB). The key thing about `MDataFrames` is that they represent the *description of data and processes applied to the data*, but do not contain data itself. In this sense `MDataFrame` is the same for MongoDB as SQL is for a relational database: a query language.\n",
    "\n",
    "\n",
    "* Like Pandas `DataFrame`, `MDataFrame` provides convenient operations for querying, subsetting, and grouping data. The key difference is that while Pandas must always load all data into memory before operating on it, `MDataFrame` passes operations on to the database. The result of such operations is typically a Pandas `DataFrame`, or another `MDataFrame`. \n",
    "\n",
    "\n",
    "* For cases where the we need more complex processing than the database supports, `MDataFrame` can run a Python function on the data in parallel.\n",
    "\n",
    "\n",
    "* Note that `MDataFrame` *is not* a drop-in replacement for Pandas `DataFrame`.\n",
    "\n",
    "### Concepts\n",
    "\n",
    "* `MDataFrame`: reprents a columnar datastructure made of of columns and rows\n",
    "* `MSeries`: represents all values in a single column\n",
    "\n",
    "In addition there are several helpers that provide specific functionality, e.g. for positional access, slicing and group-by processing. We don't specify the details of these helpers as they are tied into the MDataFrame and MSeries API.\n",
    "\n",
    "* `.value`  resolves the result of all operations, e.g. filtering, slicing, aggregation to a local in-memory pandas dataframe (useful for exploratory tasks and groupby aggregation)\n",
    "* `.apply()` provides in-database processing (e.g. filtering, slicing, aggregation)\n",
    "* `.transform()` provides parallel out-of-core, chunk-by-chunk processing (use for large datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing data as an MDataFrame\n",
    "\n",
    "You can store any of the following objects and return them as an MDataFrame:\n",
    "\n",
    "* Pandas Dataframe (and any source that Pandas supports): `om.datasets.put(df, 'name')`\n",
    "* Any sql source supported by the (e.g. snowflake) `om.datasets.put('connstr', 'name')`\n",
    "* CSV files from an externally hosted source (ftp, http, s3): `om.datasets.put(None, 'name',uri='http://....')`\n",
    "* Any other tabular-like data that you insert into the analytics store (i.e. MongoDB): `om.datasets.put(docs, 'name')`\n",
    "\n",
    "Note in general the MDataFrame is not dependent on the original *source* of the data, but on the format it is stored in omega-ml's analytics storage. As long as the data can be retrieved back and transformed to a format Pandas can work with, MDataFrame will be able to handle it. That said, it works easiest with tabular data of rows and columns where each cell has some scalar value.\n",
    "\n",
    "### Getting an MDataFrame\n",
    "\n",
    "The following methods are equivalent, both return an instance of `MDataFrame`\n",
    "\n",
    "```\n",
    "mdf = om.datasets.getl('name')\n",
    "mdf = om.datasets.get('name', lazy=True)\n",
    "```\n",
    "\n",
    "Using any of these methods, `mdf` will represent the `MDataFrame` instance. Note that this will return immediately as no data access happens at this time.\n",
    "\n",
    "#### Executing a query\n",
    "\n",
    "To actually get data from a MDataFrame you need to ask for evaluation. This will execute the query according to all operations applied so far. The result is a standard Pandas DataFrame:\n",
    "\n",
    "```\n",
    "mdf.value\n",
    "```\n",
    "\n",
    "**Note this can be a dangerous operation as it will load all the data into memory** If the result of your query is larger than the available memory of your process, it will fail and result in an operating-system level out of memory condition. If you are unsure how many rows a query will return, try using `.count()` first.\n",
    "\n",
    "#### Persisting the result of a query\n",
    "\n",
    "To evaluate an MDataFrame without returning all data into memory, use the `.persist()` method\n",
    "\n",
    "```\n",
    "mdf.persist('name', store=om.datasets)\n",
    "```\n",
    "\n",
    "This is the equivalent of `df = mdf.value; om.datasets.put(df, 'name')`, however all operations are performed in the database, results are retrieved back to memory only if needed, and if so in small chunks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "Like Pandas DataFrame, MDataFrame can be sliced\n",
    "\n",
    "* by a set of columns: `mdf[['col1', 'col2']]` => return a MDataFrame subset to col1, col2\n",
    "* by single columns `mdf['col1']` => return a MSeries\n",
    "* by rows `mdf.iloc[start:end]` => return a MDataFrame subset to rows with index start to end. \n",
    "* by index `mdf.loc[label]` => return a MDataFrame subset to columns with corresponding labels\n",
    "* by filter `mdf[filter-mask]`=> return a MDataFrame subset to the filter mask\n",
    "\n",
    "Note that `.loc, .iloc` require the data to have been stored from a Pandas DataFrame.\n",
    "\n",
    "### Filtering\n",
    "\n",
    "*By filter masks*\n",
    "\n",
    "```\n",
    "flt = mdf['column'] == value  # use any operator supported by MSeries\n",
    "mdf[flt]\n",
    "```\n",
    "\n",
    "Filtering can be done by using a combination of `keyword__<operator>=<value` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation and transformation\n",
    "\n",
    "MDataFrame provides a powerful set of aggregations:\n",
    "\n",
    "* in-database or local groupby processing `mdf.groupby`\n",
    "* in-database transformation and aggregation `mdf.apply()`\n",
    "* out-of-core parallel processing `mdf.transform()`\n",
    "\n",
    "#### In-database transformations\n",
    "\n",
    "Using `MDataFrame.apply()` we can apply several column-wise transformations. Note that the function passed to apply must accept an ApplyContext. \n",
    "\n",
    "\n",
    "```\n",
    "mdf.iloc[0:1000].apply(lambda v: {\n",
    "    'key'      : v['l_orderkey'],\n",
    "    'comment': v['l_comment'].str.concat(' *'),\n",
    "    'docs': v['l_shipinstruct'].str.usplit(' '),\n",
    "    'comment_lower': v['l_shipinstruct'].str.lower(),\n",
    "    'comment_substr': v['l_shipinstruct'].str.substr(1, 5),\n",
    "    'week': v['l_shipdate'].dt.week,\n",
    "    'year': v['l_shipdate'].dt.year,\n",
    "}, inplace=True).value\n",
    "```\n",
    "\n",
    "Note: Unlike a Pandas apply which executes the function for every row or column, MDataFrame will **execute the function only once** in order to build the database query. If you want to execute Python code row-by-row, or group-by-group, use `.tranform()`, see below.\n",
    "\n",
    "##### Parallel transformations\n",
    "\n",
    "MDataFrame supports in-parallel processing of arbitrary subsets and size of data. By default, the subset will be by row number, but any other grouping is possible.\n",
    "\n",
    "The following snipped will start N / chunksize tasks and process them in parallel. Each task processes N / chunksize records. The default chunksize is 50'000. The number of parallel jobs started by default is CPU count - 1. \n",
    "\n",
    "\n",
    "```\n",
    "def myproc(df):\n",
    "    df['column'] = df['other'].apply(...)\n",
    "    \n",
    "mdf.transform(myproc).persist('name', store=om.datasets)\n",
    "```\n",
    "\n",
    "\n",
    "More explanations:\n",
    "\n",
    "```\n",
    "def myproc(df, i):\n",
    "    # df is the subset of the ith chunk of the full data\n",
    "    # it is a Pandas in-memory DataFrame, apply any Pandas function you like\n",
    "    # assignment is supported\n",
    "    df['column'] = df['other'].apply(...)\n",
    "    ...\n",
    "    # groupby is also possible\n",
    "    result = df.groupby(...)...\n",
    "    # either return None (or no return statement) => updated df is written to the db\n",
    "    # or return a DataFrame or a Series => returned object is written to the db\n",
    "    return result\n",
    "\n",
    "# this will start N = len(mdf) / 50'000 tasks and store the results in om.datasets\n",
    "# conceptually this is the equivalence of df = mdf.value.apply(myproc); om.datasets.put(df)\n",
    "# however using mdf.transform() will use much less memory and easily scale out of core\n",
    "mdf.transform(myproc).persist('name', store=om.datasets)\n",
    "\n",
    "# specify chunksize and n_jobs to influence the number of chunks and the number of parallel workers. \n",
    "# note this comes at a trade-off: many workers will take longer to complete, larger chunksizes will use more memory\n",
    "mdf.transform(myproc, chunksize=<#records>, n_jobs=#numbers).persist('name', store=om.datasets)\n",
    "```\n",
    "\n",
    "####  Customized chunking\n",
    "\n",
    "By default `.transform()` uses the size of the data (as in number of rows) do determine the number of chunks. You can however create any number chunks:\n",
    "\n",
    "```\n",
    "mdf = om.datasets.getl('retail')\n",
    "\n",
    "def process(ldf):\n",
    "    ldf['comments'] = ldf['l_comment'].str.split(' ')\n",
    "\n",
    "\n",
    "def chunker(mdf, chunksize, maxobs):\n",
    "    # for each chunk yield a MDataFrame subset for each chunk\n",
    "    # note: don't use .value before yielding as this would resolve the dataframe locally\n",
    "    #       and potentially consume all memory. \n",
    "    groups = mdf['l_returnflag'].unique().value\n",
    "    for group in groups:\n",
    "        for i in range(0, maxobs, chunksize):\n",
    "            yield mdf.skip(i).head(chunksize).query(l_returnflag=group)\n",
    " \n",
    "(mdf\n",
    " .transform(process, chunkfn=chunker, n_jobs=-2)\n",
    " .persist('retail-transformed', store=om.datasets))\n",
    "```\n",
    "\n",
    "\n",
    "#### What won't work\n",
    "\n",
    "*MDataFrame are currently read-only. In other words, assignment, column additions and smilar operations are not currently supported. This is not an inherent restriction, there is just no API for it in the current implementation. Note if updates are required, the MDataFrame plugin mechanism provides a straight-forward way to provide such functionality.*\n",
    "\n",
    "Hence the following kind of operations are **not currently supported**:\n",
    "\n",
    "```\n",
    "mdf[col] = mdf[col].apply(func)\n",
    "mdf[col] = mdf[col].map(func)\n",
    "mdf[col] = value # partial support is available, but limited to scalar values\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
